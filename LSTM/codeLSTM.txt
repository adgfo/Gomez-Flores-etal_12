###### Thank you for looking at our code and please cite our work when you use it, thanks #######

### 1. Database (you can find the database in our repository https://github.com/adgfo/Gomez-Flores-etal_12) ###
import pandas as pd # pd version 2.1.4

df = pd.read_csv('Gomez-Flores_et_al_Database1.csv') # load database 1
df = df[['t', 'rpm', 'dp', 'db', 'rhop', 'thetap', 'mu', 'EB_bp', 'Jb', 'U1', 'U2', 'ep1', 'ep2', 'Sb', 'kp', 'Pk', 'Rp', 'Re']] # reorganize columns

### 2.1. Data processing ###
from sklearn.preprocessing import MinMaxScaler # scikit-learn version 1.3.0

xScaler = MinMaxScaler(feature_range=(0, 1)) # define x scaler
yScaler = MinMaxScaler(feature_range=(0, 1)) # define y scaler
x = xScaler.fit_transform(df[['t', 'rpm', 'dp', 'db', 'rhop', 'thetap', 'mu', 'EB_bp', 'Jb']]) # Scaling of input x
y = yScaler.fit_transform(df[['U1', 'U2', 'ep1', 'ep2', 'Sb', 'kp', 'Pk']]) # Scaling of target y. Rp and Re are ignored because their scale is already 0-1
x = x.reshape(x.shape[0], 1, x.shape[1]) # Reshape input x to input in LSTM

### 2.2. Insert Rp and Re in target y ###
import numpy as np # np version 1.26.2

y = np.append(y, df[['Rp', 'Re']], axis=1) # Insert Rp and Re in the scaled target y

### 3. LSTM modeling ###
import tensorflow as tf # tensorflow version 2.10.1
from tensorflow import keras # keras version 2.10.0
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Dense

size = 1729
model=Sequential()
model.add(LSTM(units=819, return_sequences=True, input_shape=(x[:size,:].shape[1], x[:size,:].shape[2]))) # First layer (input layer)
model.add(Dropout(0.3))
model.add(LSTM(units=819, return_sequences=True)) # Second layer
model.add(Dropout(0.3))
model.add(LSTM(units=819, return_sequences=False)) # Third layer
model.add(Dropout(0.3))
model.add(Dense(units=y[:size,:].shape[1], activation='linear', kernel_initializer='random_normal', bias_initializer='random_normal')) # Fourth layer (output layer)
model.compile(optimizer='adam', loss='mean_squared_error') # Compile LSTM

loss_t_histories = []
loss_v_histories = []
for i in range(5):
    ### 3.1. Train
    history = model.fit(x[:size,:], y[:size,:], epochs=1400, batch_size=x[:size,:].shape[0], validation_data=(x[size:,:], y[size:,:]), shuffle=False, verbose=0)
    ### 3.2. Test
    loss_v = model.evaluate(x[size:,:], y[size:,:], verbose=0)    
    ### 3.3. Loss histories
    loss_t_histories.append(history.history['loss'])
    loss_v_histories.append(history.history['val_loss'])

### 4. Validation database
toPred = pd.read_csv('Gomez-Flores_et_al_Database2.csv') # (database in our repository https://github.com/adgfo/Gomez-Flores-etal_12) # Load database 2 for validation
toPred['R'] = toPred['R']/100
xNew = xScaler.transform(toPred[['t', 'rpm', 'dp', 'db', 'rhop', 'thetap', 'mu', 'EB_bp', 'Jb']]) # Scaling of input x in validation database
xNew = xNew.reshape(xNew.shape[0], 1, xNew.shape[1]) # Reshape input x in validation database

### 5. Model validation
yNew = model.predict(xNew) # Make a prediction
yPred = yScaler.inverse_transform(yNew[:, [0, 1, 2, 3, 4, 5, 6]]) # Inverse scaling of predicted output y, Rp and Re are ignored
yPred = np.append(yPred, yNew[:, [7,8]], axis=1) # Insert Rp and Re in predicted output y
R = yPred[:, -2]+yPred[:, -1] # Calculate R = Rp + Re
